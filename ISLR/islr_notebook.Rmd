---
title: "Paul's ISLR Notebook"
author: "Paul Jeffries"
output: github_document
date: "`r Sys.Date()`"
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
require(dplyr)
```

Beginning of Paul's notebook.

# Simple Linear Regression

* Master list of slides and videos: http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/
* YT video for this section: https://www.youtube.com/watch?v=WjyuiK5taS8&list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy
* Slides accompany each set of videos. 

This section focuses on simple linear regression, multivariate linear regression, and fancier variations thereof. 

For this section we're going to use the *Boston* dataset. We'll build a variety of simple (and slightly more complicated linear models). We'll build some plots that help us to better understand the utiility of our modls, and look at some options worth exploring and ways of functionalizing linear regression code. 

```{r}
require(MASS)
require(ISLR)

names(Boston) #shows variable list of dataset
```

<br>
Next we'll move to do some preliminary plotting of the data.

```{r}
par(mfrow=c(1,1)) #ensures only one chart is plotted per page.
# par sets the parameers for the graphics display. The default is mfrow = c(2,2)
# (medv is the dependent variable and lstat is the independnt variable)
plot(medv~lstat,Boston) #plots medv modeled by lstat
```

```{r}
fit1 <- lm(medv~lstat,Boston) #fits the linear model 'lm' with same variables as above
summary(fit1) #shows p-values, coefficients, st.error, t.value, etc.
```

```{r}
par(mfrow=c(2,2)) #sets display back to default
plot(fit1) # plots all standard regression plots
```

```{r}
names(fit1) #shows elements of model summary that can be called indvidually
```

```{r}
fit1$coefficients #shows example of how to call individual elemnets from model
```

```{r}
confint(fit1) #prints only confidence interval info for model
```

```{r}
# uses the model to predict outcome variable for new inputs
# confidence interval below for 'lstat' input of 5,10,15, etc.
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
```

<br>

# Multivariate Linear Regression


```{r}
# 2-variable multiple linear regression
# dependent var = medv, ind. vars = lsta, age
fit2 <- lm(medv~lstat+age,data=Boston)
summary(fit2)
```

```{r}
# now we can fit the model with all variables in Boston dataset
# the period is short-hand for all variables in the dataset
fit3 <- lm(medv~.,Boston)
summary(fit3)
```

```{r}
par(mfrow=c(2,2)) #ensures the graphical layout is as we want for our window
plot (fit3) #show our summary regression plots for the all-variable model
```

```{r}
# Now we can update the model to remove variables with low explanatory power
# From our summary, age and indus have higher p-values, so they can be dropped
# update function updates model by getting rid of variables and keeping all others
fit4 <- update(fit3,~.-age-indus)
summary(fit4)
```

<br>

# Nonlinear Terms and Interactions

```{r}
# model below fits a model with same dependent variable: medv
# indepent vars: age, lstat, and an lstat/age interaction term
fit5 <- lm(medv~lstat*age,Boston)
summary(fit5)
```

```{r}
# we can also model ind. variables in quadratic form
# here we one of our ind. vars to a 2nd degree polynomial
fit6 <- lm(medv~lstat + I(lstat^2),Boston)
summary(fit6)
```

```{r}
# standard regression plots for the quadratic model
par(mfrow=c(2,2))
plot(fit6)
```

```{r}
par(mfrow=c(1,1)) # makes it so that all four charts produced next output to same window
#below is the ploted fit6 quadratic model overlayed on the scatterplot of mdev modeled by lstat
#as can be seen, the fit appears to be quite accurate
plot(Boston$medv~Boston$lstat)
# we'll plot the points in red for visibility
points(Boston$lstat,fitted(fit6),col="red",pch=20)
```


```{r}
# there are easier ways to fit higher degree polynomial models
# the function poly() does this and includes descending polynomials too
fit6 <-lm(medv~poly(lstat,5),data=Boston)
summary(fit6)
```

```{r}
# we can also use logged independent variables rather simply too, as below
# here we have logged rm as our ind. varible, medv as our dependent 
fit7 <- lm(medv~log(rm),data=Boston)
summary(fit7)
```

Now we can also include qualitative predictors in our model as well

```{r}
# for this we'll use a different dataset "Carseats"
summary(Carseats)
```

```{r}
# the model below fits a multivaraite regression model including all ind. vars
# it also includes two interaction effects variables
fit8 <- lm(Sales~.+Income:Advertising+Age:Price,Carseats)
# in the output below we can see that in some cases R automatically creates dummy vars
# for example ShelveLocGood being a dummy variable for qualitative ranking of "good"
summary(fit8)
```

```{r}
# in order to see how coding for qualitative variables is done we can use "contrasts"
contrasts(Carseats$ShelveLoc)
```

```{r}
# finally, we can use what we learned to functionalize regression plot creation
regplot <- function(x,y,...){
  fit=lm(y~x)
  plot(x,y,...) # allows for user-entered options here
  abline(fit,col="red")
}

# below we use this simple function, along with a few added options
regplot(Carseats$Price,Carseats$Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
```

```{r}
# we can change up the various plot point types for our charts easily, a sample is below
# below are some of the main pch types used in simple plots 
plot(1:20,1:20,pch=1:20,cex=2)
```


<br>

# Logistic Regression

```{r}
# Loading packages
require(ISLR) # grabs the datasets we need
require(MASS) # statistics and econometrics package
require(class) # used primarily for KNN analysis
```

```{r}
# Exploring the data we'll work with (Stock Market Data)
# other methods of dataset exploration include names(), summary(), and dim()
head(Smarket)
```

```{r}
# To further explore the data, we can build a correlation matrix
# We drop the last column becauuse it is string and will otherwise throw an error
cor(Smarket[,-9])
```

```{r}
# now we fit a logistic regression model w/ Direction as the outcome variable
# the argument (family = binomial) yields the logit
logit_fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)
summary(logit_fit) #shows p values, error, coefs, etc.
```

```{r}
# if we want just the coefficients, we can use (coef(logit_fit))
# if we want all the info about the coefficients, we use summary('model')$coef
summary(logit_fit)$coef
```

```{r}
# we can use standard subsetting syntax to show particular coefficient information
# for example, to just show the p-values, we use the code below
summary(logit_fit)$coef[,4]
```


```{r}
logit_probs <- predict(logit_fit,type="response")
# shows the first 10 predictions made by the logistic regression model
logit_probs[1:10]
```

```{r}
# We know that the outcome variable can only take the form Up or Down
# As such, we can transform our logit_probs into an UP / DOWN prediction
# First we created a fector filled with no's to overwrite with model results
logit_pred <- rep("Down",1250)
# Were model prediction is >.5, overwrite "Down" with "Up"
logit_pred[logit_probs>.5] = "Up" 
# Now we create a table with model results vs. actual results
table(logit_pred,Smarket$Direction)
```

```{r}
# The overall successful classification rate can be found by the mean
mean(logit_pred == Smarket$Direction)
```

The previous version of the logit model was run on all of our data, with no splitting into testing and training sets. This can lead to a problem of overfitting. For a more robust model, we need to re-estimate our model just on the training data (as done below), and then use our test set as validation. 

```{r}
# Now we begin testing the logit model more rigorously
# We need to test it across time with a training and a test set
train <- (Smarket$Year<2005) # training set is all years before 2005
smarket_2005 <- Smarket[!train,] # creates our test set (which is just the year 2005)
direction_2005 <- Smarket$Direction[!train] # vector of response variables for test set
```

```{r}
# Now we'll fit the same logit model from before but only using the training set
logit_fit2 <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
                  data = Smarket, family=binomial,subset=train)
# Now we can use the logit model fit on the training data to predict the 2005 response variable
# The general pattern is: fit on training set, test predictive power on test set
logit_probs2 <- predict(logit_fit2,smarket_2005,type="response")
# now we have our predictions
head(logit_probs2)
```

```{r}
# Analyzing Our Predictions
dim(smarket_2005) # gets us row and column counts for 2005 (test) stock market data
# we know from this result that we are going to need 252 responses to check
```

```{r}
logit_pred2 <- rep("Down",252) # row number we need given dim() called previously
# changes "down" to "yes" whenever predicted value greater than .5
logit_pred2[logit_probs2>0.5] <- "Up" 
# now we'll show the predicted vs. actual response variables
table(logit_pred2,direction_2005)
```

```{r}
# finally, the success rate for the new model (trained and tested more robustly)
mean(logit_pred2==direction_2005)
# here we see the model actually degraded (likey due to removal of some overfitting)
```

```{r}
# if the model still isnt' performing well, then we can consider fewer ind. variables (as below)
# fewer explanatory terms may prove more effective (hence techniques like ridge regression)
logit_fit3 <- glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)
summary(logit_fit3)
```

```{r}
# once we fit the model, we perform the same steps as done previously to test model accuracy
# we form our predictions based on the model, classify our test set observations, and check them
logit_probs3 <- predict(logit_fit3,smarket_2005,type="response")
logit_pred3 <- rep("Down",252)
logit_pred3[logit_probs3>0.5]<-"Up"
table(logit_pred3,direction_2005)
```

```{r}
# we can see as well that this smaller model with fewer terms yields a higher success rate
mean(logit_pred3==direction_2005)
```

```{r}
# now let's predict our outocme variable for particuarl new input data (beyond test set)
# we can manually predict Direction using our model for any value of our ind. variables
# below we predict Direction for 2 pairings of Lag1 and Lag 2
# as we can see, both would be predicted as Down days.
predict(logit_fit3,newdata=data.frame(Lag1=c(1.2,1.4),Lag2=c(1.1,-0.8)),type="response")
```

<br>

# Linear Discrimant Analysis

- Linear discriminant analysis is generally more sensitive to outliers than logistic regression.

```{r}
# First we'll fit the LDA model on the same data as before
lda_fit <- lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
# note that with LDA / QDA calling summary(model name) is not nearly as helpful
(lda_fit)
```

```{r}
# plots the LDA model details in histogram
plot(lda_fit)
```

```{r}
lda_pred <- predict(lda_fit, smarket_2005) # uses LDA to predict for test set of 2005
# names(lda_pred) # this would show the names of the variables output by the prediction 
lda_class <- lda_pred$class # creates a vector of the LDA model's classifications
# now we can see the same predicted vs. actual table as previously done with regression
table(lda_class,direction_2005)
```
```{r}
# shows the accuracy perecent of the basic LDA model 
mean(lda_class == direction_2005)
```

```{r}
# We can see by quick comparison that the LDA and logistic regression predictions are identical
mean(logit_pred3==direction_2005)
```

```{r}
# now we can look more deeply into the posterior probabilities
# the posterior probability output by the model = probability of market decrease
sum(lda_pred$posterior[,1]<.5) / nrow(lda_pred$posterior) # number of predictions of market increase
```

```{r}
# the method on the line below shows the probabilty of the market being up / down by observation
lda_pred$posterior[1:20,1] # shows the first 20 probabilities of teh market going down
```
```{r}
lda_class[1:20] #viewing the first 20 days of the market going down 
```

```{r}
max(lda_pred$posterior[,1])
# the greatest posterior probabilty of decrease in all 2005 was 52.05%
```
```{r}
sum(lda_pred$posterior[,1]>.9) 
# no single observation > .9 (i.e. the model is never very confident)
```

<br>

# Quadratic Discriminant Analysis

```{r}
# QDA involves a quadratic rather than a linear model
qda_fit <- qda(Direction~Lag1+Lag2,data=Smarket,subset = train)
qda_fit
```

```{r}
qda_class <- predict(qda_fit,smarket_2005)$class
# having formed our predictions, we can look at the tables success rate (skipping table)
mean(qda_class==direction_2005)
# here we see a slightly higher hit rate on our test set than with linear methods 
```

# K - Nearest Neighbors

```{r}
# creationg out test and training sets using a slightly different method
train_x <- cbind(Smarket$Lag1,Smarket$Lag2)[train,]
test_x <- cbind(Smarket$Lag1,Smarket$Lag2)[!train,]
train_direction <- Smarket$Direction[train] # outcome variable in training set
set.seed(1) # setting our seed for reproducibilty of any decisions due to randomization
knn_pred <- knn(train_x,test_x,train_direction,k=2) # training model with 2 classes # there are empirical methods of picking classes, but 2 makes sense here (up/down)
# picking a value of k=1 wouldn't make sense because we have 2 possible outcomesd
# as can be seen, the knn model perform slightly better than average
mean(knn_pred==direction_2005)
```

```{r}
# Now we can try retraining the model with a higher value of k
# We'll build a function that tests success rates for various values of k
# It will test all values of k up to and including the argument input (k_count)

k_val_test = function(k_count){
  k_results = data.frame(1:k_count,rep(0,k_count))
  colnames(k_results) <- (c("K-value","Success Rate"))
  
  for(k in 1:k_count){
    knn_pred=knn(train_x,test_x,train_direction,k=k_count)
    k_results[k,2] = mean(knn_pred==direction_2005)
  }
  
  return(k_results) # returns a dataframe of all sucess rates for each value of k
  
}

k_val_test(10)
```

<br>

# Cross-validation 

This section compares the utility of different types of cross-validation (leave-one-out vs. 10-fold). The YT video for this section can be found here:


```{r}
# bring in the necessary packages
require(ISLR)
require(boot) 

# basic plot of mpv vs. horsepower to remind us of data we'll be playing with
plot(mpg~horsepower, data=Auto)
```

<br>

## Leave-One-Out Cross Validation (LOOCV)

```{r}
# ?cv.glm shows details for the cross validation function from the glm package

# simple model of mpg using horsepower that we will cross-validate 
glm_fit <- glm(mpg~horsepower, data=Auto) 

# now we estiamte K-fold cross-validation prediction error for our glm_fit model
# the below call is pretty slow; we'll improve on it later 
cv.glm(Auto,glm_fit)$delta 
```

```{r}
# 2 outputs of cv.glm: RAW LOOCV error, and bias-corrected LOOCV error
# bias correction accounts for fact that the LOO training set is smaller than the full dataset
# the impact of this kind of bias correction is obviously more important in k-fold vs. LOOCV

# We can now write a slightly faster function for LOOCV
# It is faster because it takes into account the self-influence of each value
# (see video for additional explanation)

# Formula 5.2 from ISLR:

loocv <- function(fitted_model){
  h = lm.influence(fitted_model)$h
  LOOCV_prediction_error = mean((residuals(fitted_model)/(1-h))^2)
  answer = as.data.frame(LOOCV_prediction_error)
  return(answer)
}

# Now we test our formula
loocv(glm_fit)

```

```{r}
# Continuing with the same example data:

# 1st I create an empty vector with 5 values = 0 to change later with a loop
cv_error <- rep(0,5) 

# Then I create a 2nd vector with values 1:5 (also for upcoming loop)
degree <- 1:5

# Loop iterats through each degree of polynomial, fitting a different degree model (1:5)
# During each loop, it takes the LOOCV error and outputs it to the cv_error vector

for(d in degree){
  glm_fit = glm(mpg~poly(horsepower, d), data=Auto)
  cv_error[d] = loocv(glm_fit)
}

# Now I take the result of the loop in order to plot the error for each degree polynomial
plot(degree,cv_error,type="b", main = "LOOCV Error by Degree of Polynomial in Model")
# In this case we can see that the model prediction error drops off at poly=2
```


```{r}
# Now we can do the same thing that we did above but for 10-fold Cross-Validation 

cv_error10 <- rep(0,5) # making a new blank vector for our 10-fold CV error

for(d in degree){
  glm_fit = glm(mpg~poly(horsepower,d), data=Auto)
  cv_error10[d] = cv.glm(Auto,glm_fit,K=10)$delta[1]
}

# plots the 10-fold CV error on top of the LOOCV error rates
plot(degree,cv_error,type="b", main = "LOOCV Error by Degree of Polynomial in Model")
lines(degree,cv_error10,type="b",col="red")

# we can see that the two methods track almost identically in term of their error
# that said, the 10-fold is much less computationally intensive
# as such, especially in large-N cases, we prefer to use 10-fold CV techniques
```

<br>

# Boostrap

This sections examines the utility of the boostrap method of estimating modle prediction error. The YT video for this section can be found here.

The example case that we'll use here will be picking the optimal combination of 2 investments (investment X and investment Y).

```{r}
# we begin by brining in the standard necessary packages
require(ISLR)
require(boot)

# The function below is not the textbook financial alpha function
# Its purpose is to give us some function that we can then use with a boostrap method

alpha <- function(x,y){
  vx=var(x) # variance of x
  vy=var(y) # variance of y
  cxy=cov(x,y) # covariance of x and y
  alpha=((vy-cxy)/(vx+vy-2*cxy)) # computes alpha
  return(alpha)
  
}

alpha(Portfolio$X, Portfolio$Y)
```

```{r}
# Now we want to investigate the standard error of alpha:
# We create a function that slices as dataset and calculates the alpha within that range

alpha_fn <- function(data, index){
  with(data[index,], alpha(X,Y))
}

alpha_fn(Portfolio,1:100)
```

```{r}
# we set the seed here because boostrapping relies on some element of randomization
# always set seed if we want to maintain reproducibility 
set.seed(1315)

# here we calculate alpha for a random sample (with replacement) of 100 observations
# this version samples only from the first 100 observations 
# using all 100 observations to get alpha:
alpha_fn(Portfolio,sample(1:100,100,replace=TRUE))

# if we wanted to sample from the entire dataset, we drop the first argument 
alpha_fn(Portfolio,sample(100,replace=TRUE))
```

```{r}
# now we move on to the actual boostrapping
boot_out <- boot(Portfolio,alpha_fn,R=1000)
boot_out
```

```{r}
plot(boot_out)
```


## Estimating the Accuracy of a Linear Regression Model via Bootstrapping


```{r}

boot_fn <- function(data,index){
  return(coef(lm(mpg~horsepower,data=data,subset=index)))
}

#simply compute coefficient estimates
boot_fn(Auto,1:392)


```

```{r}
set.seed(1)

#one bootstrap round
boot_fn(Auto,sample(392,392,replace=T))

```

```{r}
#now do a thousand!
boot(Auto,boot_fn,1000)
```

```{r}
#however in the simple case of linear regression, we can also get these
# estimates with the summary() function from the fit itself
# as was described in section 3.1.2
summary(lm(mpg~horsepower,data=Auto))$coef
```



<br>

# Tree-Based Methods

(Bagging, random forest, and boosting)

https://www.youtube.com/watch?v=0wZUXtvAtDc&index=6&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh

Decision Trees
```{r}
library(ISLR)
library(tree)
attach(Carseats)
hist(Sales)
High=ifelse(Carseats$Sales<=8,"No","Yes")
Carseats=data.frame(Carseats, High) #adds the variable to dataframe
```

Now we fit a tree to these data, summarize, and plot it. Notice that we have to _exclude_ 'Sales' from the right-hand side of the formula, because the response is derived from it.

```{r}
# model high as a subset of everything except Sales
tree.carseats=tree(High~CompPrice+Income+Advertising+Population+Price,data=Carseats)
summary(tree.carseats)
```

```{r}
plot(tree.carseats)
text(tree.carseats,pretty = 0) # enables tree annotations
```

```{r}
tree.carseats #gives more detailed print out of all terminal nodes
```

Now let's make our methodology more robust by including a training and a test set (split: 250,150), grow the tree on the training set, and evaluate its performance on the test set, as per the norm.

```{r}
set.seed(1000)
# take a random sample of 250 for training set
train=sample(1:nrow(Carseats),250)
# estimate the model on the training subset
tree.carseats=tree(High~CompPrice+Income+Advertising+Population+Price,Carseats,subset=train)
# plot the tree
plot(tree.carseats);text(tree.carseats,pretty=0)
```

```{r}
tree.pred=predict(tree.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))

```

The tree was grown to full depth, and might be too variable. We now use Cross-Validation to prune it.

```{r}
cv.carseats=cv.tree(tree.carseats,FUN = prune.misclass)
cv.carseats
```

```{r}
plot(cv.carseats)
```


```{r}
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats);text(prune.carseats,pretty=0)
```

Finally, we can evaluate this pruned tree on the test data.

```{r}
tree.pred=predict(prune.carseats, Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
```
https://www.youtube.com/watch?v=IY7oWGXb77o&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh&index=7

```{r}

```






















