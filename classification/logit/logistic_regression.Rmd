---
title: "Logistic Regression"
output: github_document
---

```{r results='hide', warning=FALSE, message=FALSE}
# first a few general set-up items / housekeeping items

# setting the appropriate working directory
setwd("~/Desktop/Personal/personal_code/classification/")

# setting scipen options to kill all use of scientific notation
options(scipen = 999)

# basic packages needed throughout
library(dplyr) # for piping
library(ggplot2) # for visualization
library(ggthemes) # for custom visualization
```

# Importing, Exploring, Cleaning, Normalizing / Centering, and Prepping the Data

## Importing the Data

- Data taken from: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/
- Explanation of the meaning / origin of the data can be found in this academic paper here: http://www3.dsi.uminho.pt/pcortez/wine5.pdf
- Additional explanation of the data and corresponding Kaggle page: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009


```{r}
# we have both red and white wine datasets with the same variables 
base_red <- read.csv("data/winequality-red.csv",sep=";")
base_white <- read.csv("data/winequality-white.csv",sep=";")

# lots of useful information about the dataset
glimpse(base_red) 
glimpse(base_white) 

# the datasets both have the same variables, but the red dataset has notably fewer observations
```

```{r}
# given that the two datasets all have the same variables, we'll add a color variable to each and then combine

base_red <- base_red %>%
  mutate(color = "red")

base_white <- base_white %>%
  mutate(color = "white")

# combining the two data frames
main_wine_df <- bind_rows(base_red, base_white)

# viewing the newly combined data frame
glimpse(main_wine_df)
```

```{r}
library(janitor) # for data cleaning and tabular exploration
# documentation: https://github.com/sfirke/janitor

# first we'll do some mandatory / precautionary cleaning
# tidying variable names and dropping any useless rows / columns

main_wine_df <- main_wine_df %>%
  janitor::clean_names() %>% # converts to underscore case and cleans
  janitor::remove_empty(which = c("rows","cols")) # drops all rows and columns that are entirely empty
```

In this preliminary analysis, I will build a model only for white wine. The reason for this is entirely arbitrary--I personally love most red wine, and the only wine I have ever absolutely despised was white. As such, for my own intellectual curriosity I wanted to create a model that predicts the classification of the lowest quality white wine specifically, with an eye towards better understanding the characteristics of the worst white wine in order to avoid them altogether. 

```{r}
# for the purpose of simplicity, we are going to start by only looking at the white wine
white_main_df <- main_wine_df %>%
  # filtering to just the white wine
  filter(color == "white") %>%
  # dropping the now-useless variable
  select(-color) %>%
  # ensuring quality is a factor; this will be useful later
  # as a rule of thumb, it's good to factor any non-numeric variables when glm modeling
  mutate(quality = factor(quality))

# examining the newly created dataset
glimpse(white_main_df)
```

## Exploring the Data

```{r}
# Even though we dropped any rows / cols that are entirely null, we need to check for NA problems
library(DataExplorer) # allows for creation of missings values map
# documentation for DataExplorer: https://towardsdatascience.com/simple-fast-exploratory-data-analysis-in-r-with-dataexplorer-package-e055348d9619
DataExplorer::plot_missing(white_main_df) # shows % of NAs within each variable
```

Good news at this point is this dataset looks perfectly clean of nulls! If there were any problems with nulls, we would solve it using complete.cases() or something similar.

### Continous Variables Exploration 

```{r}
# high-level univariate variable-exploration
# first a histogram of all continuous variables in the dataset
DataExplorer::plot_histogram(data = white_main_df, title = "Continuous Variables Explored (Histograms)")
```

```{r}
# then a density chart of all continous variables in the dataset
DataExplorer::plot_density(data = white_main_df, title = "Continuous Variables Explored (Density Plots)")
```

### Categorical Variable Exploration

```{r}
# the only categorical variable in our data in this case is what we'll use to create our low quality flag
# if we had many categorical variables, it would make sense to use order_bar = TRUE
# the order would then be in descending order of prevalence, which is helpful at a glance
plot_bar(data = white_main_df, order_bar = FALSE, title = "Categorical Variables Explored")
```

```{r}
# and then we can use janitor to see the exact cross-tab of our quality variable
janitor::tabyl(white_main_df$quality)
# it looks like wines with a rating < 5 are exceptionally bad, so we'll use that as our benchmark
# all together wines with a rating below 5 represent just under 4% of the population
```

It looks like wines with a rating < 5 are exceptionally bad, so we'll use that as our benchmark. All together wines with a rating below 5 represent under 4% of the population--so we'll be dealing with a low incidence binary outcome left-hand side variable in this particular modeling scenario. 


### Outcome Variable Creation

```{r}
# given the above analysis, we'll flag anything with a quality rating < 5 as low-quality 
white_final_df <- white_main_df %>%
  # type conversion here can be tricky because to de-factor requires multiple steps
  # we have to de-factor, perform the logical test on the numeric, and then re-factor
  mutate(low_qual_flag = factor(ifelse(as.numeric(as.character(quality)) < 5,1,0))) %>%
  select(-quality)

glimpse(white_final_df) # taking another look at the new dataset
```

```{r}
# And now we'll take one final look at the distribution of our outcome variable
# as can be seen, a low quality white wine is a rare event; ~3.75% of the time
tabyl(white_final_df$low_qual_flag)
```

## Centering and Normalizing the Data 

For more information on when to center / normalize data, see below:
- https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia
- tl;dr --> center the data when you want your intercept term to represent your expectation when the model is fed the average for each variable in the model, as opposed to the model expectation when all variables == 0; normalize the data when the variable ranges differ markedly

```{r}
# we're going to scale and center all variables (except our left-hand side)
white_final_df[,-12] <- scale(white_final_df[,-12], center = TRUE, scale = TRUE)
glimpse(white_final_df)
```

## Checking for Variable Correlations

For more on all the cool varieties of correlation plots that can be created, see below:
- https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html

```{r}
# package needed for all varieties of correlation plots
library(corrplot)

# building a correlation matrix and ensuring that it only takes in components that are numeric 
# this is necessary because if there are any non-numeric elements in the matrix, this will break 
corr_matrix <- cor(white_final_df[, sapply(white_final_df, is.numeric)])

# getting the matrix of p-values that correspond to the strength of correlation for each pairing
res1 <- cor.mtest(white_final_df[, sapply(white_final_df, is.numeric)], conf.level = .95)

# first we'll build a correlation plot that checks for significance level
# this one will also give us a hint at strength of correlation based on the colors 
corrplot(corr_matrix, p.mat = res1$p, method = "color", type = "upper",
         sig.level = c(.001, .01, .05), pch.cex = .9,
         insig = "label_sig", pch.col = "black", order = "AOE", na.label = "NA")

# and finally we'll build a simpler corrplot to get the strenth of correlation numbers visualized
corrplot(corr_matrix, method = "number", type = "upper", pch.cex = .9,
         order = "AOE", number.cex = .7, na.label = "NA")
```

Takeways from these types of exploratory techniques can help us to create a more informed model. We may, depending on the circumstances, treat variables differently in our model-building process as a result of these types of charts. For example, we might discover a great degree of cross-correlation that allows us to delete duplicative variables, etc. We can notice a few interesting trends from our results above in this case:

- Residual sugar and denisty are highly positively correlated, while alcohol and density are highly negatively correlated. These correlations are also significant. While most of these are intuitive, we might do well to remember these cross-correlations when it comes time to do dimensionality reduction with this model (if necessary). All in all, it seems as though there are a few interesting relationships to be explored here, but nothing that appears concerning from the perspective of very high degree of multicolinearity. 


## Prepping Data for the Modeling Process

```{r}
# split the data into training and testing sets
library(caret) # needed to createDataPartitions

# Partition data: 80 / 20 split : train / test
# set seed to ensure reproducibility
set.seed(777)

in_train <- caret::createDataPartition(y=white_final_df$low_qual_flag, p=0.80, list=FALSE)

# splits the data into training and testing sets
training <- white_final_df[in_train,]
testing <- white_final_df[-in_train,]

# shows the row count and column count of the training and test sets, to check that all worked as planned
dim(training)
dim(testing)
```

# Building a Basic Logit 

## Estimating the Model 

```{r}
# simple logistic regression
# models using all variables in the training dataset (hence ~ .)
simple_logit_fit <- glm(low_qual_flag ~ .,
                 data = training,
                 family = binomial)

summary(simple_logit_fit)
```

## First Look at Model Predictions 

```{r}
# first we'll examine what sort of predictions the model would make when fed the training set
# then we'll repeat this with the testing set (which we should care a bit more about)
# then we'll observe the distribution of modelled probabilities to look for interesting trends

# run predictions on training set
prediction_train <- predict(simple_logit_fit, newdata = training, type = "response" )
predictions_train_full <- data.frame(prediction = prediction_train, low_qual_flag = training$low_qual_flag)

# run predictions on testing set
prediction_test <- predict(simple_logit_fit, newdata = testing, type = "response" )
predictions_test_full <- data.frame(prediction = prediction_test, low_qual_flag = testing$low_qual_flag)

# distribution of the prediction score grouped by known outcome (for training set only)
ggplot(predictions_train_full, aes(prediction_train, color = as.factor(training$low_qual_flag) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
theme_economist()

# distribution of the prediction score grouped by known outcome (for testing set only)
ggplot(predictions_test_full, aes(prediction_test, color = as.factor(testing$low_qual_flag) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
theme_economist()
```

## Determining What Classification Cutoff is Appropriate (Simple Logit)

```{r}
# some custom functions are sourced in, to reduce document's length
# the majority of these functions are from ethen8181's GitHub, with edits / improvements I added
# more info on these custom functions here: http://ethen8181.github.io/machine-learning/unbalanced/unbalanced.html

# sourcing my adopted version of the aforementioned functions directly from my GitHub
library(RCurl) # Provides functions to allow one to compose general HTTP requests, etc. in R
# grabbing the raw info from my GitHub to turn into a text object
script <- getURL("https://raw.githubusercontent.com/pmaji/r-stats-and-modeling/master/classification/useful_classification_functions.R", ssl.verifypeer = FALSE)
# sourcing that code just like you might source an R Script locally
eval(parse(text = script))

# using newly-sourced function AccuracyCutoffInfo to test for optimal cutoff visually
accuracy_info <- AccuracyCutoffInfo(train = predictions_train_full, 
                                    test = predictions_test_full, 
                                    predict = "prediction", 
                                    actual = "low_qual_flag",
                                    # iterates over every cutoff value from 1% to 99% 
                                    # steps in units of 10 bps
                                    cut_val_start = 0.01,
                                    cut_val_end = 0.99,
                                    by_step_size = 0.001)

# from the plot below we can begin to eyeball what the optimal cutoff might be 
accuracy_info$plot
```

```{r}
# Moving on to using ROC Curves to pinpoint optimal cutoffs

# user-defined costs for false negative and false positive to pinpoint where total cost is minimized
cost_fp <- 10 # cost of false positive
cost_fn <- 100 # cost of false negative
# here the assumption I've made is that a false positive is 1/10th as costly as a false negative

# creates the base data needed to visualize the ROC curves
roc_info <- ROCInfo(data = predictions_test_full, 
                    predict = "prediction", 
                    actual = "low_qual_flag", 
                    cost.fp = cost_fp, 
                    cost.fn = cost_fn )
```

### ROC Curve for Simple Logit 

```{r fig2, fig.height = 4, fig.width = 6, fig.align = "center"}
# plot the roc / cutoff-selection plots
# color on the chart is cost -- darker is higher cost / greener is lower cost
grid.draw(roc_info$plot)
```

It looks like, for the Simple Logit, the optimal cutoff is 0.07. 


## Examining Model Performance for the Simple Logit

```{r}
# visualize a particular cutoff's effectiveness at classification
cm_info <- ConfusionMatrixInfo(data = predictions_test_full, 
                               predict = "prediction", 
                               actual = "low_qual_flag", 
                               cutoff = .07) # (determined by roc_info$plot above)

# prints the visualization of the confusion matrix (use print(cm_info$data) to see the raw data)
cm_info$plot
```

```{r}
# lastly, we'll use the cutoff we have arrived at from the work above to test the model's predictions
# think of this section as the cross-tab version of the confusion matrix plot shown above

# getting model probabilities for our testing set
simple_logit_fit_probs <- predict(simple_logit_fit,
                           newdata = testing,
                           type = "response")

# turning these probabilities into classifications using the cutoff determined above 
simple_logit_fit_predictions <- factor(ifelse(simple_logit_fit_probs > 0.07, 1, 0),levels=c('0','1'))

# builiding a confusion matrix 
simple_logit_conmatrix <- caret::confusionMatrix(simple_logit_fit_predictions,testing$low_qual_flag, positive='1')
simple_logit_conmatrix
```

## Documenting Performance of Simple Logit Model

```{r}
# creating a blank table to record a running set of performance metrics for each model we create
running_model_synopsis_table <- data.frame(
  model_name = character(),
  classification_cutoff = numeric(),
  sensitivity=character(), 
  specificity=character(), 
  number_of_model_terms=numeric(),
  total_cost = numeric(),
  stringsAsFactors=FALSE
  )

simple_logit_synopsis_info <- data.frame(
  model_name = "Simple Logit (All Vars.)",
  classification_cutoff = roc_info$cutoff,
  sensitivity = percent(roc_info$sensitivity), 
  specificity = percent(roc_info$specificity), 
  number_of_model_terms = (length(colnames(simple_logit_fit$qr$qr))-1),
  total_cost = roc_info$totalcost,
  stringsAsFactors=FALSE
  )

running_model_synopsis_table <- bind_rows(running_model_synopsis_table, simple_logit_synopsis_info)
running_model_synopsis_table
```



# Penalized Logistic Regression (Lasso)

- Now using an Objective Function that penalizes low-ROI variables. This is similar to ridge regression except variables with coefficients non-consequential enough will be zero'ed out of the model.
- Useful source: http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/

## Tuning the Hyperparameter for the Lasso Model w/ 2-Way Interactions and Polynomial Terms

The first step here is optmizing lamda--the hyperparameter of importance in a lasso regression. Given the nature of the cross-validation involved in a the hyperparameter optimization process, we need a dataset where the data aren't as scarce as they are in the main dataset. There are a variety of methods available to deal with this problem, including upsampling, downsampling, and a host of synthetic sampling methodologies that generally into categories abbreviated ROSE and SMOTE. For information on all varieties of synthetic sampling that exist, see the link below. 

- https://cran.r-project.org/web/packages/smotefamily/smotefamily.pdf

For the purpose of this project, we will estimate two lasso models:
- One trained on data created via simple upsampling.
- And one trained on data created via DBSMOTE--a dbscan-backed version of synthetic upsampling. 

The reason for these two choices is that upsampling is the simplest method for this type of problem, and DBSMOTE is the fanciest synthetic data creation methodology that makes sense for this type of problem--so we're testing the two extremes of possible solutions to the data paucity problem. 

Finally, it may be worth stating explicitly that my purpose of using the lasso method is to get an idea for what variables might be useful, and what variables might not be useful. As such, it makes sense to throw many variables at the lasso model to take full advantage of its capacity to weed out the noise. For this exercise, I will consider all of the following in each lasso model:

- All single variable terms
- All nth-degree polynomial terms (where n is bounded by 3 for simplicity's sake)
- All unique two-direction interaction effects 

After training the models using both aforementioned sampling methodologies, we will hopefully be able to glean some information in the end about what polynomial terms, interaction effects, or single variables appear most statistically powerful and thus worthy of consideration in the final model. 


## Sampling Methodology Explored -- Upsampling

### Building the Upsampled Dataset

```{r}
# creating new training dataset with upsampled low quality cases
upsample_training <- caret::upSample(training, (training$low_qual_flag))

# one downside of the upSample function is it creates a new left-hand side variable (class)
# as such, we have to do some minor clenaing

upsample_training <- upsample_training %>%
  select(-low_qual_flag) %>%
  rename(`low_qual_flag` = `Class`)

# then we inspect the incidence rate of our left-hand-side variable in the new training set
# the result should now be a 50 / 50 split 
janitor::tabyl(upsample_training$low_qual_flag) 
```

## Building the Model Formula (Upsampled Lasso)

```{r}
# first we get all variables with their corresponding degree polynomial into the model
# I'm also including here all two:way interaction effects
# in the code below, the integer in quotes determines the max nth degree polynomial to be tested
upsample_lasso_formula <- as.formula(
  paste('low_qual_flag ~', paste('poly(', colnames(upsample_training[-12]),',3)', collapse = ' + '), '+ .^2', '- .')
  )

# prints the formula so that we can see what will be used to create the logit
upsample_lasso_formula
```


### Building the Upsampled Model Matrix

```{r}
# Then we build our model matrix (including all two-way interactions possible and polynomials up to 3rd degree)
x <- model.matrix(upsample_lasso_formula, upsample_training)
# calling out what the outcome variable should be explicitly for this method
y <- upsample_training$low_qual_flag
```

### Tuning Lambda for Upsampled Logit

```{r echo=T, results='hide', warning=FALSE}
library(glmnet) # package needed for ridge methods 
# Next we move on to find the best lambda using cross-validation
# Cross-validation is for tuning hyperparameters; not normally needed if model requires no hyperparameters
set.seed(777) # set seed for reproduciblity
# alpha = 1 just means lasso ; alpha = 0 is ridge
# this step below can take a long time, as the range of possible lambdas is simulated
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
```


### Plotting Lambdas for Upsampled Logit

```{r}
# plots the various possible lambdas 
plot(cv.lasso)
# Two common choices for lambda: lambda min and lambda lse (both are shown with dotted lines, in turn)
# Up to the modeler to decide which to choose in this case; simplicity / error-minimization tradeoff
```

The two common choices are shown visually with dotted lines, in turn:
- Lambda Min (the value that minimizes the prediction error)
- Lambda LSE (gives the simplest model but also lies within one SE of the optimal value of lambda)


### Examining the Resultant Coefficients (Upsampled Lasso)

The number of non-zero'ed out variables that you will get as the result of tuning your lambda for your particular lasso model can vary greatly from model-to-model, depending on a host of variables including your training dataset. Given that this is the first time the coefficient list has showed up in this document, we'll include both the lambda min coefficient list and the lamda lse coefficient list below, but thereafter will only use the one deemed most appropriate. This choice is left up the modeler and is balancing act between exhaustiveness and simplicity, in most cases. 

### First the Coefficients for the Lambda Min Upsampled Lasso

```{r}
# lambda min is the value that minimizes the prediction error
# cv.lasso$lambda.min  # uncomment and call this if you wanted the raw lambda 

# showing coefficients using lambda min (if . then variable has been zero'ed out)
coef(cv.lasso, cv.lasso$lambda.min)
```

### Second the Coefficients for the Lambda LSE Upsampled Lasso

```{r}
#cv.lasso$lambda.1se # uncomment and call this if you wanted the raw lambda 
# showing coefficients using lambda min (if . then variable has been zero'ed out)
coef(cv.lasso, cv.lasso$lambda.1se)

# storing the coefficients for later use
upsample_lambda_coefs <- broom::tidy(coef(cv.lasso, cv.lasso$lambda.1se))
```

### Building the Upsampled Lasso Logit Formula Based on Coefficient List

```{r}
# first we trim from the coefficient list only the coefficients that weren't 0'ed out
upsample_coef_list <- upsample_lambda_coefs %>%
  # arrange the coefficients in descending order of absolute value
  arrange(desc(abs(value))) %>% 
  select(row) %>%
  # dropping the intercept which isn't needed in formula
  filter(row != '(Intercept)') %>%
  as.data.frame()

# then we take this jumbled list and need to perform a few annoying operations to get it into a clean formula
# adding unique at the end because once we drop the polynomial number at the end, we'll have some dupes
clean_upsample_coef_list <- gsub(").*", ")", upsample_coef_list$row) %>% unique()

# the gsub above resolts in a clean character vector, but now we need to make it into a formula
result_upsample_lasso_formula <- as.formula(
  paste('low_qual_flag ~', paste(clean_upsample_coef_list, collapse = ' + '))
  )

# the final resulting formula is below
result_upsample_lasso_formula
```

### Rebuilding Logit Based on Outcome of Upsample Lasso Selection Methodology

```{r}
# rebuilt logit based on info gained from lasso; would like to be able to simply plug in the coef list, from above
result_upsample_lasso_fit <- glm(result_upsample_lasso_formula,
                 data = training,
                 family = binomial)

summary(result_upsample_lasso_fit)
```


### Re-Determining What Classification Cutoff is Appropriate (Upsample Lasso)

```{r}
# running predictions on the new post-lasso-improvements-integrated model
# same chunks of code used previously below; 1st to find best cutoff, then to test performance

# run predictions on testing set
prediction_test <- predict(result_upsample_lasso_fit, newdata = testing, type = "response" )
predictions_test_full <- data.frame(prediction = prediction_test, low_qual_flag = testing$low_qual_flag)

# again defining the costs of false positive vs. costs of false negative (same ratio maintained)
cost_fp <- 10
cost_fn <- 100

# building the data structure needed for the ROC charts
roc_info <- ROCInfo(data = predictions_test_full, 
                    predict = "prediction", 
                    actual = "low_qual_flag", 
                    cost.fp = cost_fp, 
                    cost.fn = cost_fn )
```

### ROC Curve for Logit Resuliting from Upsample Lasso

```{r fig4, fig.height = 4, fig.width = 6, fig.align = "center"}
# plot the new roc / cutoff-selection plots
grid.draw(roc_info$plot)
```

Looks like the optimal cutoff for this particular model is 0.13. 


### Examining Model Performance (Upsample Lasso)

```{r}
# getting model probabilities for our testing set 
result_upsample_lasso_fit_probs <- predict(result_upsample_lasso_fit,
                           newdata = testing,
                           type = "response")

# turning these probabilities into classifications using the cutoff determined above 
result_upsample_logit_fit_predictions <- factor(ifelse(result_upsample_lasso_fit_probs > 0.13, 1, 0),levels=c('0','1'))

# builiding a confusion matrix 
result_upsample_conmatrix <- caret::confusionMatrix(result_upsample_logit_fit_predictions,testing$low_qual_flag, positive='1')
result_upsample_conmatrix
```

## Documenting Performance of Upsample Lasso Model

```{r}
upsample_lasso_synopsis_info <- data.frame(
  model_name = "Upsample Lasso",
  classification_cutoff = roc_info$cutoff,
  sensitivity = percent(roc_info$sensitivity), 
  specificity = percent(roc_info$specificity), 
  number_of_model_terms = (length(colnames(result_upsample_lasso_fit$qr$qr))-1),
  total_cost = roc_info$totalcost,
  stringsAsFactors=FALSE
  )

# adding the model at hand's metrics to our running table for continous comparison
running_model_synopsis_table <- bind_rows(running_model_synopsis_table, upsample_lasso_synopsis_info)
running_model_synopsis_table
```


## Sampling Methodology Explored -- DBSMOTE

### Building the DBSMOTE Dataset

```{r echo=T, results='hide', warning=FALSE}
library(smotefamily) # main SMOTE variety package
library(dbscan) #needed for dbsmote type of SMOTE to function

# first we construct a SMOTE-built training dataset that is more well-balanced than our actual pop. 
dbsmote_training <- smotefamily::DBSMOTE(training[,-c(12)], as.numeric(as.character(training$low_qual_flag)))
# then we inspect the incidence rate of our left-hand-side variable in the new training set
```

```{r}
# as with the upsampled dataset we created, the smote dataset requires some cleaning
dbsmote_training <- dbsmote_training$data %>%
  as.data.frame() %>%
  mutate(low_qual_flag = factor(class)) %>%
  select(-class)
  
# then we inspect the incidence rate of our left-hand-side variable in the new training set
# the result should now be close to a 50 / 50 split 
janitor::tabyl(dbsmote_training$low_qual_flag) 
```


## Building the Model Formula (DBSMOTE Lasso)

```{r}
# first we get all variables with their corresponding degree polynomial into the model
# I'm also including here all two:way interaction effects
# in the code below, the integer in quotes determines the max nth degree polynomial to be tested
dbsmote_lasso_formula <- as.formula(
  paste('low_qual_flag ~', paste('poly(', colnames(dbsmote_training[-12]),',3)', collapse = ' + '), '+ .^2', '- .')
  )

# prints the formula so that we can see what will be used to create the logit
dbsmote_lasso_formula
```

### Building the DBSMOTE Model Matrix

```{r}
# Then we build our model matrix (including all two-way interactions possible and polynomials up to 3rd degree)
x <- model.matrix(dbsmote_lasso_formula, dbsmote_training)
# calling out what the outcome variable should be explicitly for this method
y <- dbsmote_training$low_qual_flag
```

### Tuning Lambda for DBSMOTE Lasso

```{r echo=T, results='hide', warning=FALSE}
# Next we move on to find the best lambda using cross-validation
# Cross-validation is for tuning hyperparameters; not normally needed if model requires no hyperparameters
set.seed(777) # set seed for reproduciblity
# alpha = 1 just means lasso ; alpha = 0 is ridge
# this step below can take a long time, as the range of possible lambdas is simulated
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
```

### Plotting Lambdas for DBSMOTE Logit

```{r}
# plots the various possible lambdas 
plot(cv.lasso)
# Two common choices for lambda: lambda min and lambda lse (both are shown with dotted lines, in turn)
# Up to the modeler to decide which to choose in this case; simplicity / error-minimization tradeoff
```


### First the Coefficients for the Lambda Min DBSMOTE Lasso

```{r}
# lambda min is the value that minimizes the prediction error
# cv.lasso$lambda.min  # uncomment and call this if you wanted the raw lambda 

# showing coefficients using lambda min (if . then variable has been zero'ed out)
coef(cv.lasso, cv.lasso$lambda.min)
```

### Second the Coefficients for the Lambda LSE DBSMOTE Lasso

```{r}
#cv.lasso$lambda.1se # uncomment and call this if you wanted the raw lambda 
# showing coefficients using lambda min (if . then variable has been zero'ed out)
coef(cv.lasso, cv.lasso$lambda.1se)

# storing the coefficients for later use
dbsmote_lambda_coefs <- broom::tidy(coef(cv.lasso, cv.lasso$lambda.1se))
```


### Building the DBSMOTE Lasso Logit Formula Based on Coefficient List

```{r}
# first we trim from the coefficient list only the coefficients that weren't 0'ed out
dbsmote_coef_list <- dbsmote_lambda_coefs %>%
  # arrange the coefficients in descending order of absolute value
  arrange(desc(abs(value))) %>% 
  select(row) %>%
  # dropping the intercept which isn't needed in formula
  filter(row != '(Intercept)') %>%
  as.data.frame()

# then we take this jumbled list and need to perform a few annoying operations to get it into a clean formula
clean_dbsmote_coef_list <- gsub(").*", ")", dbsmote_coef_list$row) %>% unique()

# the gsub above resolts in a clean character vector, but now we need to make it into a formula
result_dbsmote_lasso_formula <- as.formula(
  paste('low_qual_flag ~', paste(clean_dbsmote_coef_list, collapse = ' + '))
  )

# the final resulting formula is below
result_dbsmote_lasso_formula
```

### Rebuilding Logit Based on Outcome of DBSMOTE Lasso Selection Methodology

```{r}
# rebuilt logit based on info gained from lasso; would like to be able to simply plug in the coef list, from above
result_dbsmote_lasso_fit <- glm(result_dbsmote_lasso_formula,
                 data = training,
                 family = binomial)

summary(result_dbsmote_lasso_fit)
```


### Re-Determining What Classification Cutoff is Appropriate (DBSMOTE Lasso)

```{r}
# running predictions on the new post-lasso-improvements-integrated model
# same chunks of code used previously below; 1st to find best cutoff, then to test performance

# run predictions on testing set
prediction_test <- predict(result_dbsmote_lasso_fit, newdata = testing, type = "response" )
predictions_test_full <- data.frame(prediction = prediction_test, low_qual_flag = testing$low_qual_flag)

# again defining the costs of false positive vs. costs of false negative (same ratio maintained)
cost_fp <- 10
cost_fn <- 100

# building the data structure needed for the ROC charts
roc_info <- ROCInfo(data = predictions_test_full, 
                    predict = "prediction", 
                    actual = "low_qual_flag", 
                    cost.fp = cost_fp, 
                    cost.fn = cost_fn )
```

### ROC Curve for Logit Resuliting from DBSMOTE Lasso

```{r fig5, fig.height = 4, fig.width = 6, fig.align = "center"}
# plot the new roc / cutoff-selection plots
grid.draw(roc_info$plot)
```

Looks like the optimal cutoff for this particular model is 0.07. 


### Examining Model Performance (DBSMOTE Lasso)

```{r}
# getting model probabilities for our testing set 
result_dbsmote_lasso_fit_probs <- predict(result_dbsmote_lasso_fit,
                           newdata = testing,
                           type = "response")

# turning these probabilities into classifications using the cutoff determined above 
result_dbsmote_lasso_fit_predictions <- factor(ifelse(result_dbsmote_lasso_fit_probs > 0.07, 1, 0),levels=c('0','1'))

# builiding a confusion matrix 
result_dbsmote_conmatrix <- caret::confusionMatrix(result_dbsmote_lasso_fit_predictions,testing$low_qual_flag, positive='1')
result_dbsmote_conmatrix
```

## Documenting Performance of DBSMOTE Lasso Model

```{r}
dbsmote_lasso_synopsis_info <- data.frame(
  model_name = "DBSMOTE Lasso",
  classification_cutoff = roc_info$cutoff,
  sensitivity = percent(roc_info$sensitivity), 
  specificity = percent(roc_info$specificity), 
  number_of_model_terms = (length(colnames(result_dbsmote_lasso_fit$qr$qr))-1),
  total_cost = roc_info$totalcost,
  stringsAsFactors=FALSE
  )

# adding the model at hand's metrics to our running table for continous comparison
running_model_synopsis_table <- bind_rows(running_model_synopsis_table, dbsmote_lasso_synopsis_info)
running_model_synopsis_table
```


# Final Model Selection (Progress Thus Far)

Having created various models over the course of this analysis, we can take stock of where we are and compare what we have thusfar. From the running_model_synopsis_table, shown above, we see that the DBSMOTE Lasso has the best sensitivity of all models. That said, our end goal should not just be to have a relatively predicitve model, but one that has some practical utility in the real world. As such, we'll look over the DBSMOTE Lasso and try to simplify it bit to end with something that is both easily explainable and practical. 

## Trimming the Best Model Found Thus Far

We want to take the best model we have found thus far and trim its terms down to only those that are statistically significant, such that the final model is both simpler and more robust. 

### Functionalized Model-Trimming Based on P-Values

```{r}
library(broom) # needing for tidying model summary output into a df

# turns model summary for dbsmote lasso model into df
tidy_dbsmote_lasso <- broom::tidy(result_dbsmote_lasso_fit)

final_coef_list <- tidy_dbsmote_lasso %>%
  # arrange the terms in ascending order of p-value
  arrange(abs(p.value)) %>% 
  # dropping the intercept which isn't needed in formula
  # keeping only variables that are statistically significant at a 95% CI
  filter(
    term != '(Intercept)',
    p.value < 0.05
         ) %>%
  as.data.frame()

# then we take this jumbled list and need to perform a few annoying operations to get it into a clean formula
clean_final_coef_list <- gsub(").*", ")", final_coef_list$term) %>% unique()

# turns stat. significant list of final coefficients into a callable formula
result_final_lasso_formula <- as.formula(
  paste('low_qual_flag ~', paste(clean_final_coef_list, collapse = ' + '))
  )

# the final resulting formula is below
result_final_lasso_formula
```

```{r}
result_final_lasso_fit <- glm(result_final_lasso_formula,
                 data = training,
                 family = binomial)

summary(result_final_lasso_fit)
```

### Manual Model-Trimming for Final Model

```{r}
trimmed_final_lasso_fit <- glm(low_qual_flag ~ volatile_acidity + poly(free_sulfur_dioxide,3) + alcohol + fixed_acidity,
                 data = training,
                 family = binomial)

summary(trimmed_final_lasso_fit)
```

## Checking Model Performance of Trimmed Final Lasso Model

```{r}
# run predictions on testing set
prediction_test <- predict(trimmed_final_lasso_fit, newdata = testing, type = "response" )
predictions_test_full <- data.frame(prediction = prediction_test, low_qual_flag = testing$low_qual_flag)

# user-defined costs for false negative and false positive to pinpoint where total cost is minimized
cost_fp <- 10 # cost of false positive
cost_fn <- 100 # cost of false negative
# here the assumption I've made is that a false positive is 1/10th as costly as a false negative

# creates the base data needed to visualize the ROC curves
roc_info <- ROCInfo(data = predictions_test_full, 
                    predict = "prediction", 
                    actual = "low_qual_flag", 
                    cost.fp = cost_fp, 
                    cost.fn = cost_fn )

```

### ROC Curve for Trimmed Final Lasso

```{r fig17, fig.height = 4, fig.width = 6, fig.align = "center"}
# plot the roc / cutoff-selection plots
# color on the chart is cost -- darker is higher cost / greener is lower cost
grid.draw(roc_info$plot)
```

Looks like the recommended cutoff is at 0.10. 

```{r}
# getting model probabilities for our testing set 
result_final_lasso_fit_probs <- predict(result_final_lasso_fit,
                           newdata = testing,
                           type = "response")

# turning these probabilities into classifications using the cutoff determined above 
result_final_lasso_fit_predictions <- factor(ifelse(result_final_lasso_fit_probs > 0.10, 1, 0),levels=c('0','1'))

# builiding a confusion matrix 
final_final_conmatrix <- caret::confusionMatrix(result_final_lasso_fit_predictions,testing$low_qual_flag, positive='1')
final_final_conmatrix
```

## Documenting Performance of Trimmed Final Lasso Model

```{r}
trimmed_final_lasso_synopsis_info <- data.frame(
  model_name = "Trimmed Final Lasso",
  classification_cutoff = roc_info$cutoff,
  sensitivity = percent(roc_info$sensitivity), 
  specificity = percent(roc_info$specificity), 
  number_of_model_terms = (length(colnames(trimmed_final_lasso_fit$qr$qr))-1),
  total_cost = roc_info$totalcost,
  stringsAsFactors=FALSE
  )

# adding the model at hand's metrics to our running table for continous comparison
running_model_synopsis_table <- bind_rows(running_model_synopsis_table, trimmed_final_lasso_synopsis_info)
running_model_synopsis_table
```

# Conclusions

While this is a continual work-in-progress, much has been accomplished thus far. We started with a basic model using all variables in the dataset, and then, using more advanced methods like DBSMOTE sampling and Lasso regression, arrived at a model with relatively high sensitivity and specificity given our pre-selected cost preferences. Finally, we reduced the dimensionality and complexity of that most predictive model, rendering it more easy to explain and apply in practice. It appears as though, when it comes to classifiying bad white wines--the lower the levels of free sulfur dioxide, the higher the odds are that the wine in question is poor quality. 


Yet to come:
- Variable importance commentary

## Summary of Chosen "Best" Model 

```{r}
summary(trimmed_final_lasso_fit)
```

```{r}

```


